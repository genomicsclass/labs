---
layout: page
title: Cross-validation
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

In this lab, we will explore a method for picking parameters in a
prediction / machine learning task, which is called
*cross-validation*.

Suppose we have a prediction algorithm which is going to predict the
class of some observations using a number of features. For example, we
will use the gene expression values to predict the tissue type in our
tissues gene expression dataset.

If this algorithm has a parameter which controls the behavior, we
might pick the value of this parameter which minimizes the
classification error. However, trying to classify the same
observations as we use to *train* the model can be misleading.
In lecture, we saw that for K-nearest neighbors, using k=1 will always
give 0 classification error in the training set (because we use the
single observation to classify itself). Instead, it's better to pick
the parameter using the algorithms performance on a set of
observations which the algorithm has never seen, a *test* set.

Cross-validation is simply a method which splits the data into a
number of *folds*. If we have N folds, then the algorithm typically
trains on (N-1) of the folds, and test the algorithms performance on
the left-out single fold. This is then repeated N times until each
fold has been used as a *test* set.

Let's load in the tissue gene expression dataset:

```{r}
# library(devtools)
# install_github("dagdata","genomicsclass")
library(dagdata)
data(tissuesGeneExpression)
library(Biobase)
rownames(tab) <- tab$filename
t <- ExpressionSet(e, AnnotatedDataFrame(tab))
t$Tissue <- factor(t$Tissue)
colnames(t) <- paste0(t$Tissue, seq_len(ncol(t)))
```

Let's drop one of the tissues which doesn't have many samples:

```{r}
library(class)
table(t$Tissue)
t <- t[,t$Tissue != "placenta"]
t$Tissue <- droplevels(t$Tissue)
table(t$Tissue)
x <- t(exprs(t))
```

We will use the `createFolds` function from the `caret` 
package to make 5 folds of the data, which are
balanced over the tissues. Don't be confused that the 
`createFolds` function uses the same letter 'k' as the k in 
K-nearest neighbors. These 'k' are unrelated. 
The caret function `createFolds` is
asking for how many folds to create, the 'N' from above. The `knn`
function is asking how many closest observations to use to classify
the test observations.

```{r}
# install.packages("caret")
library(caret)
set.seed(1)
idx <- createFolds(t$Tissue, k=5)
sapply(idx, function(i) table(t$Tissue[i]))
```

Now we can try out the K-nearest neighbors method on a single fold:

```{r}
pred <- knn(train = x[ -idx[[1]], ], test = x[ idx[[1]], ], cl=t$Tissue[ -idx[[1]] ], k=5)
table(true=t$Tissue[ idx[[1]] ], pred)
```

As the prediction is looking too good in the space of all the genes,
let's make it more difficult for the K-nearest neighbors algorithm.
We will use a reduced dimension representation of the dataset, using
the *multi-dimensional scaling* algorithm used in the previous section.

```{r}
xsmall <- cmdscale(dist(x))
```

Now we will create a loop, which tries out each value of k from 1 to
12, and runs the K-nearest neighbors algorithm on each fold. We then
ask for the proportion of errors for each fold, and report the average
from the 5 cross-validation folds:

```{r}
set.seed(1)
ks <- 1:12
res <- sapply(ks, function(k) {
  # try out each version of k from 1 to 12
  
  res.k <- sapply(seq_along(idx), function(i) {
    # loop over each of the 5 cross-validation folds

    # predict the held-out samples using k nearest neighbors
    pred <- knn(train = xsmall[ -idx[[i]], ],
                test = xsmall[ idx[[i]], ],
                cl = t$Tissue[ -idx[[i]] ], k = k)

    # the ratio of misclassified samples
    mean(t$Tissue[ idx[[i]] ] != pred)
  })
  
  # average over the 5 folds
  mean(res.k)
})
```

Now we can plot the mean misclassification rate for each value of k:

```{r}
plot(ks, res, type="o")
```
