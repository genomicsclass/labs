---
layout: page
title: Factor Analysis
---

```{r options, echo=FALSE}
library(knitr)
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

# Introduction 

Many of the statistical ideas applied to correcting for batch effects come from Factor Analysis. Factor Analysis has was first developed over a century ago. Pearson noted that the between subject grades were correlated between subjects when the correlation was computed across students. To explain this he posed a model having one factor that was common across subjects for each student that explained this correlation:

$$
Y_ij = \alpha_i W_1 + \varepsilon_{ij}
$$
with $Y_{ij}$ the grade for individual $i$ on subject $j$ and $\alpha_i$ representing the ability of student $i$ to obtain good grades. 

In this example, $W_1$ is a constant. Here we will motivate factor analysis with a slightly more complicated situation that resembles the presence of batch effects. We generate random grande
$\mathbf{Y}$ is $N \times 6$ are grades in five different subjects for N children. 

```{r,echo=FALSE}
library(MASS)
library(rafalib)
n <- 250
p <- 6
set.seed(1)
g <- mvrnorm(n,c(0,0),matrix(c(1,0.5,0.5,1),2,2))
Ystem <- g[,1] + matrix(rnorm(n*p/2,0,0.65),n,p/2)
Yhum <- g[,2] + matrix(rnorm(n*p/2,0,0.65),n,p/2)
Y <- cbind(Ystem,Yhum)
colnames(Y) <- c("Math","Science","CS","Eng","Hist","Classics")
```

# Sample correlations

Note we observe high correlation across five subject:
```{r}
round(cor(Y),2)
```

A graphical look shows that the correlation suggest a grouping into STEM and humanities.

In the figure below high correlations are red, no correlation is white and negative correlations are blue.

```{r,fig.align="center",echo=FALSE,fig.width=14}
mypar2(1,2)
cols=colorRampPalette(rev(brewer.pal(11,"RdBu")))(100)
eps = matrix(rnorm(n*p),n,p)
par(mar = c(8.1, 8.1, 3.5, 2.1))


image(1:ncol(Y),1:ncol(Y),cor(Y)[,6:1],xaxt="n",yaxt="n",col=cols,xlab="",ylab="",zlim=c(-1,1),main="Actual Data")
axis(1,1:ncol(Y),colnames(Y),las=2)
axis(2,1:ncol(Y),rev(colnames(Y)),las=2)

image(1:ncol(Y),1:ncol(Y),cor(eps)[,6:1],xaxt="n",yaxt="n",col=cols,xlab="",ylab="",zlim=c(-1,1),main="Independet Data")
axis(1,1:ncol(Y),colnames(Y),las=2)
axis(2,1:ncol(Y),rev(colnames(Y)),las=2)

```


# Factor model

Based on the plot above we hypothesize that there are two hidden factors $\mathbf{W}_1$ and $\mathbf{W}_2$ and to account for the observed correlation structure we model the data in the following way:

$$
Y_{ij} = \alpha_{i,1} W_{1,j} + \alpha_{i,2} W_{2,j} + \varepsilon_{ij}
$$

The interpretation of these parameters are as follows: $\alpha_{i,1}$ is the overall ability for student $i$ and $\alpha_{i,2}$ is the  difference in ability between the two subgroups for student $i$. Can we estimate the $W$ and $\alpha$? 

# Factor analysis and PCA

The first two principal components estimate $W_1$ and $W_2$ [we need to add reference for the math]

```{r}
s <- svd(Y)
W <- t(s$v[,1:2])
colnames(W)<-colnames(Y)
round(W,1)
```

Note that, as expected, the first factor is close to a constant and will help explain the observed correlation across all subjects, while the second is a factor that differs between STEM and humanities and 
We can use these estimate in the model:
$$
Y_{ij} = \alpha_{i,1} \hat{W}_{1,j} + \alpha_{i,2} \hat{W}_{2,j} + \varepsilon_{ij}
$$
 
and we can now fit the model:

```{r}
fit = s$u[,1:2]%*% (s$d[1:2]*W)
var(as.vector(fit))/var(as.vector(Y))
```


# Factor Analysis in General

In high throughput data is is quite common to see correlation structure. For example, notice the complex correlations we see across samples in the plot below. These are the correlations for a gene expression experiment with columns ordered by data:

```{r,fig.align="center",message =FALSE}
library(Biobase)
library(GSE5859)
data(GSE5859)
n <- nrow(pData(e))
o <- order(pData(e)$date)
Y=exprs(e)[,o]
cors=cor(Y-rowMeans(Y))

mypar2(1,1)
cols=colorRampPalette(brewer.pal(9,"Blues"))(100)
par(mar = c(8.1, 8.1, 3.5, 2.1))
image(1:n,1:n,cors,xaxt="n",yaxt="n",col=cols,xlab="",ylab="",zlim=c(-1,1))
```

Two factors will not be enough to model the observed correlation structure. But a more general factor model can be useful:

$$
Y_{ij} = \sum_{k=1}^K \alpha_{i,k} W_{j,k} + \varepsilon_{ij}
$$

And we can use PCA to estimate $\mathbf{W}_1,\dots,\mathbf{W}_K$. Choosing $k$ is a challenge and in the next section we describe how exploratory data analysis might help.


