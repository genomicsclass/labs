---
layout: page
title: Exploratory Data Analysis
---

use purl to extract the code
setwd("C:\\Users\\Xiaodong\\Documents\\GitHub\\labs\\week1")
use TAB to find the file in working directory
library(knitr)
purl("exploratory_data_analysis.Rmd")


```{r options, echo=FALSE}
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```
Biases, systematic errors and unexpected variability are common in genomics data. Failure to discover these problems often leads to flawed analyses and false discoveries. As an example, consider that experiments sometimes fail and not all data processing pipelines are designed to detect these. Yet, these pipelines still give you an answer and the from the final results it may be hard or impossible to notice an error was made. In later modules we will cover many other examples.

Graphing data is a powerful approach to detecting these problems. We refer to this as _exploratory data analyis_ (EDA). Many important methodological contributions to genomics data analysis were initiated as discovery made via EDA. We will show some useful exploratory plots for gene expression data measured with microarrays and NGS. We start with a general introduction to EDA using height data.
## Histograms

We can think of any given dataset as a list of numbers. Suppose you have measured the heights of all men in a population. Imagine you have need to describe these numbers to someone that has no idea what these heights are, say an alien that has never visited earth.


```{r definingHeights, message=FALSE}
library(UsingR)
x=father.son$fheight
length(x)

```
One approach is to simply list out all the numbers for the alien to see. Here are 20 randomly selected heights of 1,078.

```{r}
round(sample(x,20),1)
```

From scanning through these numbers we start getting a rough idea of what the entire list looks like but it certainly inefficient. We can quickly improve on this approach by creating bins, say by rounding each value to its nearest integer, and reporting the number of individuals in each bin. A plot of these heights is called a histogram
```{r histogram, fig.width=4, fig.height=4}
hist(x,breaks=seq(floor(min(x)),ceiling(max(x))),main="",xlab="Height")
```
Showing this plot to the alien is much more informative than showing the numbers. Note that with this simple plot we can approximate the number of individuals in any given interval. For example, there are about 70 individuals over six feet (72 inches) tall. 

ecdf(x) gives an empirical CDF, showing the distribution of height 

```{r}
xs<-seq(floor(min(x)),ceiling(max(x)),0.1)
plot(xs,ecdf(x)(xs),type="l",xlab="x=Height",ylab="F(x)")
```

## Normal approximation

If instead of the total numbers we report the proportions, then the histogram is a probability distribution. The probability distribution we see above approximates one that is very common in a nature: the bell curve or normal distribution or Gaussian distribution. When the histogram of a list of numbers approximates the normal distribution we can use a convenient mathematical formula to approximate the proportion of individuals in any given interval

$$
\mbox{Pr}(a < x < b) = \int_a^b \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left( \frac{-(x-\mu)^2}{2 \sigma^2} \right)} \, dx
$$

Here $\mu$ and $\sigma$ are refereed to as the mean and standard deviation. If this approximation holds for our list then the population mean and variance of our list can be used in the formula above. To see this with an example remember that above we noted that 70 individuals or 6% of our population were taller than 6 feet. The normal approximation works well, since the proportion consistently agree with the theoritical calculation:
```{r}
mean(x>72)
1-pnorm(72,mean(x),sd(x)) 
mean(x>70)
1-pnorm(70,mean(x),sd(x))
mean(x<70)
pnorm(70,mean(x),sd(x))

mean(x<65)
pnorm(65,mean(x),sd(x))
```

A very useful characteristic of this approximation is that one only needs to know $\mu$ and $\sigma$ to describe the entire distribution. All we really have to tell our alien friend is that heights follow a normal distribution with average height 68'' and a standard deviation of 3''. From this we can compute the proportion of individuals in any interval. 

## QQ-plot

To corroborate that the normal distribution is in fact a good approximation we can use quantile-quantile plots. Quantiles are best understood by considering the special case of percentiles. The p-th percentile of a list of a distribution is defined as the number q that is bigger than p% of numbers. For example, the median 50-th percentile is the median. We can compute the percentiles for our list and for the normal distribution
```{r}
ps <- seq(0.01,0.99,0.01)
qs <- quantile(x,ps)
normalqs <- qnorm(ps,mean(x),sd(x))
plot(normalqs,qs,xlab="Normal percentiles",ylab="Height percentiles")
abline(0,1) ##identity line
```
Note how close these values are. Also note that we can do same with less code
```{r}
qqnorm(x)
qqline(x)
```

## Box-plot
Data is not always normally distributed. Income 
In these cases the average and standard deviation are not necessarily informative. The properties described above are specific to the normal. For example, the normal distribution does not seem to be a good approximation for the direct compensation for 199 United States CEOs in the year 2000

Note the flat right tail of QQ plot

```{r fig.width=4, fig.height=4}
hist(exec.pay)
qqnorm(exec.pay)
qqline(exec.pay)
```
A practical summary is to compute 3 percentiles: 25-th, 50-th (the median) and the 75-th. A boxplots shows these 3 values along with a range calculated as median $\pm$ 1.5 75-th percentiles - 25th-percentile. Values outside this range are shown as points.

```{r fig.width=4, fig.height=4}
boxplot(exec.pay,ylab="10,000s of dollars",ylim=c(0,400))
```

## Scatter plots and correlation
In the biomedical sciences it is common to be interested in the relationship between two or more variables. A classic examples is the father/son height data used by Galton to understand the heredity. Were we to summarize these data we could use the two average and two standard deviation as both are well approximated by the normal distribution. This summary however fails to describe an important characteristic of the data.

```{r fig.width=4, fig.height=4}
data("father.son")
x=father.son$fheight
y=father.son$sheight
plot(x,y,xlab="Father's height in inches",ylab="Son's height in inches",main=paste("correlation =",signif(cor(x,y),2)))
```
The scatter plot shows a general trend: the taller the father the taller to son. A summary of this trend is the correlation coefficient which in this cases is 0.5. We motivate this statistic by trying to predict son's height using the father's.


## Stratification
Suppose we are asked to guess the height of randomly select height from the sons. The average height, 68.7 inches, is the value with the highest proportion (see histogram) and would be our prediction. But what if we are told that the father is 72 inches tall, do we sill guess 68.7?

Note that the father is taller than average. He is 1.7 standard deviations taller than the average father. So should we predict that the son is also 1.75 standard deviations taller? Turns out this is an overestimate. To see this we look at all the sons with fathers who are about 72 inches. We do this by _stratifying_ the son heights.
```{r fig.width=6, fig.height=3}
boxplot(split(y,round(x)))
print(mean(y[ round(x) == 72]))

sapply(split(y,round(x)),mean)

```
Stratification followed by boxplots lets us see the distribution of each group. The average height of sons with fathers that are 72 is 70.7. We also see that the means of the strata appear to follow a straight line. 

This line is refereed to the regression line and it's slope related to the correlation. When two variables follow a bivariate normal distribution then for any given value of x we predict the value of y with
$$
\frac{Y - \mu_Y}{\sigma_Y} = r \frac{X-\mu_X}{\sigma_X}
$$
with the $\mu$ representing the averages, $\sigma$ the standard deviations, and $r$ the correlation. Let's compare the mean of each strata to the identity line and the regression line

```{r fig.width=3, fig.height=3}
x=(x-mean(x))/sd(x)
y=(y-mean(y))/sd(y)
means=tapply(y,round(x*4)/4,mean)
fatherheights=as.numeric(names(means))
plot(fatherheights,means,ylab="average of strata of son heights",ylim=range(fatherheights))
abline(0,cor(x,y))
```

One interpretation of the correlation: slope of the line for normalized bivariate normal distribution
two means, two sd's, corr are good summary of bivariate normal

## Spearman's correlation
Just like the average and standard deviation are not good summaries when the data is not well approximated by the normal distribution, the correlation is not a good summary when pairs of lists are not approximated by the bivariate normal distribution. Examples include cases in which on variable is related to another by a parabolic function. Another, more common example are caused by outliers or extreme values.

```{r fig.width=3, fig.height=3}
a=rnorm(100);a[1]=10
b=rnorm(100);b[1]=11
plot(a,b,main=paste("correlation =",signif(cor(a,b),2)))

a=rnorm(100)
b=rnorm(100)
plot(a,b,main=paste("correlation =",signif(cor(a,b),2)))

```
In the example above the data are not associated but for one pair both values are very large. The correlation here is about 0.5. This is driven by just that one point as taking it out lowers to correlation to about 0. An alternative summary for cases with outliers or extreme values is Spearman's correlation which is based on ranks instead of the values themselves. 
```{r fig.width=3, fig.height=3}
set.seed(1)
a=rnorm(100);a[1]<-25
b=rnorm(100);b[1]<-26
plot(a,b,main=paste("cor_spearman=",round(cor(a,b,method="spearman"),2),
                    "corr=",round(cor(a,b),2)))
```
Outlier affect sample average and sd. Outliers  are common in high throughput measurement. Robust statistics are preferable. but in some cases of inference robust statisticses may lead to lesser sensitivity
MAD (median absolute deviation)  is a robust estimate of sd. 
median(abs(x-median(x))) where x is a vector of data points. Multiply this by 1.4824 to make it unbiased. 


```{r}
x<-rnorm(100)
x[sample(1:100,1)]<-100
boxplot(x,main=paste("Mean ",signif(mean(x),1),", SD ",signif(sd(x),1),
                     "\n","Median ",signif(median(x),1),"MAD ", 
                     signif(mad(x),1)))
```
Spearman correlation: use ranks and calculate the correlations. 
```{r}
x<-rnorm(100)
y<-rnorm(100)
n<-sample(1:100,1)
x[n]<-100
y[n]<-80
par(mfrow=c(1,2))
plot(x,y,main=paste("corr=",round(cor(x,y),2)))
plot(rank(x),rank(y),main=paste("spearman=",round(cor(x,y,method="spearman"),2)))
```
Log Transformation
--------
###Mean-Variance dependence for an RNA-seq dataset: plot mean across the samples 
vs. sd across the samples. higher mean, higher variance
###Sums and averages are driven by highly expressed genes:
Max of a sample affects the average of that sample
--take the log then compute the average and variance, make the mean-variance relationship flatter
###Symmetry of log ratios
log transformation also makes fold change symmetrical 
```{r fig.width=3, fig.height=3}
x<-seq(from=-5,to=5)
y<-2^x
plot(x,y,main="Ratios are not symmetric around 1",
     cex=0.5,col="blue",pch=19,ylim=c(0,40))
text(x,y,fractions(y),pos=3)
plot(x,y,log="y",main="Log ratios are symmetric around 0",
     cex=0.5,col="blue",pch=19)
text(x,y,fractions(y),pos=3)
```




In the example above the data are not associated but for one pair both values are very large. The correlation here is about 0.5. This is driven by just that one point as taking it out lowers to correlation to about 0. An alternative summary for cases with outliers or extreme values is Spearman's correlation which is based on ranks instead of the values themselves. 



