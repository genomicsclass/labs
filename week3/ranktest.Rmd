---
layout: page
title: Rank tests
---

```{r options, echo=FALSE}
opts_chunk$set(fig.path=paste0("figure/", sub("(.*).Rmd","\\1",basename(knitr:::knit_concord$get('infile'))), "-"))
```

## Wilcoxon signed-rank test

The normal apporximatiion is often useful when analyzing 
high throughput data. However, due to the complexity of the measurement devices it is also common to mistakenly  observe data points generated by an undesired process. For example, a defect on a scanner can produce a hanful of very high intensities. Thus we may have situationst that are approximated by a, for example, 99 data points from a standard normal distribution and one very large number.

```{r}
set.seed(1)
N=25
B=10000
X<-matrix(rnorm(N*B,0,1),B,N)
Y<-matrix(rnorm(N*B,0,1),B,N)
Y[,1]<-5;Y[,2]<-7 ##two outliers
ps=sapply(1:B,function(i){
  return(c(t.test(X[i,],Y[i,])$p.value,
  wilcox.test(X[i,],Y[i,])$p.value))
  }
)
mypar(1,2)
hist(ps[1,],nc=20,xlab="p-values",ylim=c(0,1100),main="t-stat")
hist(ps[2,],nc=20,xlab="p-values",ylim=c(0,1100),main="Wilcoxon")
```

```{r}
i <- which.max((ps[2,]-ps[1,])*(ps[1,]<0.05))
x=X[i,]
y=Y[i,]
stripchart(list(x,y),vertical=TRUE,ylim=c(-7,7),ylab="Observations",pch=21,bg=1,cex=1.25)
abline(h=0)
xrank<-rank(c(x,y))[seq(along=x)]
yrank<-rank(c(x,y))[-seq(along=y)]
stripchart(list(xrank,yrank),vertical=TRUE,ylab="Ranks",pch=21,bg=1,cex=1.25)
ws <- sapply(x,function(z) rank(c(z,y))[1]-1)
text(rep(1.05,length(ws)),xrank,ws)
W <-sum(ws) 
n1<-length(x);n2<-length(y)
Z <- (mean(ws)-n2/2)/ sqrt(n2*(n1+n2+1)/12/n1)
```

```{r}
stripchart(y-x,vertical=TRUE,ylim=c(-8,8))
abline(h=0)
stripchart(rank(abs(y-x))*sign(y-x),vertical=TRUE,ylim=c(-25,25))
abline(h=0)
W=sum(abs(rank(abs(y-x))*sign(y-x)))
```


```{r}
tab=cbind(sign(y-x),abs(y-x),rank(abs(y-x)))
tab=tab[order(tab[,2]),]
```




In statistics we refer these types of points as _outliers_. A small number of outliers can throw of an entire analysis. For example notice how this one point results in the sample mean and sample variace being very far from the 0 and 1 respectively.
```{r}
mean(x)
sd(x)
```
The median, defined as the point having half the data larger and half the data smaller, is summary statistic that is _robust_ to outliers. Note how much close the median is to 0, the center of out actual distribution. 
```{r}
median(x)
```

The median absolute deviace (MAD) is a robust summary for the standard deviation. It is defined by computing the differences between each point and the median and taking the median of their absolute values:
$$
 1.4826 \mbox{median}\{| X_i - \mbox{median}(X_i)|\}
$$
The number $1.4826$ is a scale factor that guarantees an unbiased 
estimate of the actual center. Notice how much closer we are to one with the mad:
```{r}
mad(x)
```

## Spearman correlation
The correlation is also sensitive to outliers. Here we construct a independent list of numbers but for which a simialr mistake was made for the same entry:

```{r}
set.seed(1)
x=c(rnorm(100,0,1)) ##real distribution
x[23] <- 100 ##mistake made in 23th measurement
y=c(rnorm(100,0,1)) ##real distribution
y[23] <- 84 ##similar mistake made in 23th measurement
library(rafalib)
mypar(1,1)
plot(x,y,main=paste0("correlation=",round(cor(x,y),3)),pch=21,bg=1,xlim=c(-3,100),ylim=c(-3,100))
abline(0,1)
```

## Rank tests

We use the same data as in previous module. We have data for two strains of mice which we will refer to as strain 0 and 1. We want to know which genes are differentially expressed.  We extracted RNA from 12 randomely selected mice from each strain [CITE POOLING PAPER]. In one experiment we pooled the RNA from all individuals from each strain and then created 4 replicate samples from this pool. 

```{r}
library(Biobase,quietly=TRUE,verbose=FALSE)
library(genefilter)
library(dagdata)
data(maPooling)
pd=pData(maPooling)
individuals=which(rowSums(pd)==1)
##remove replicates
individuals=individuals[-grep("tr",names(individuals))]
y=exprs(maPooling)[,individuals]
group=factor(as.numeric(grepl("b",names(individuals))))
```

We can compute the sample variance for each gene and compare to the standard deviation obtained with the technical replicates.
```{r}
ind1<-which(group==1);ind0<-which(group==0)
wilcox<-apply(y,1,function(x){
    tmp<-wilcox.test(x[ind1],x[ind0],exact=FALSE)
  c(tmp$statistic,tmp$p.value)
})
ttest<-genefilter::rowttests(y,group)
ind<-which(ttest$p.val<0.05 & wilcox[2,]>0.05)
mypar(3,3)
for(i in ind)
  stripchart(split(y[i,],group),vertical=TRUE,jitter=0.2)
ind
```

Note the biological variance is much larger than the technical one. And also that the variability of variances is also for biological variance. Here are the two genes we showed above but now for each individual mouse 

```{r, }
mypar(1,2)
stripchart(split(y[i,],group),vertical=TRUE,method="jitter",col=c(1,2),xlab="Gene 1",pch=15)
points(c(1,2),tapply(y[i,],group,mean),pch=4,cex=1.5)
stripchart(split(y[j,],group),vertical=TRUE,method="jitter",col=c(1,2),xlab="Gene 2",pch=15)
points(c(1,2),tapply(y[j,],group,mean),pch=4,cex=1.5)
```

Note the p-value tell a different story
```{r}
library(genefilter)
tt=rowttests(y,group)
tt$p.value[i]
tt$p.value[j]
```

Which of these two genes do we feel more confident reporting as being differentially expressed? If another investigator takes another random sample of mice and tries the same experiment, which one do you think will replicate? Measuring biological vairability is essential if we want our conclusions to be about the strain of mice in general as opposed to the specific mice we have. 

Now which genes do we report as stastitically significant? For somewhat arbitrary reasons, in science p-values of 0.01 and 0.05 are used as cutoff. In this particular example we get 

```{r}
sum(tt$p.value<0.01)
sum(tt$p.value<0.05)
```


## Multiple testing
But do we report all these genes? Let's explore what happens if we split the first group into two, forcing the null hypothesis to be true

```{r}
set.seed(0)
shuffledIndex <- factor(sample(c(0,1),sum(group==0),replace=TRUE ))
nullt <- rowttests(y[,group==0],shuffledIndex)
sum(nullt$p.value<0.01)
sum(nullt$p.value<0.05)
```
If we use the 0.05 cutoff we will be reporting 840 false postives. That's a lot! In a later module we will learn about _multiple testing_ corrections. In the meantime now that p-values lose their meaning when we are combing through a long list of tests for the largest ones. An important statistical fact to know is that when the null hypothesis is true for independent tests and we compute the p-value for each, then the distribution of these p-values is uniform: any interval between 0 and 1 has the same proportion. 

```{r}
mypar(1,2)
hist(tt$p.value,xlab="p-values",main="p-values from experiment",freq=FALSE,ylim=c(0,4),col=3)
hist(nullt$p.value,xlab="p-values",main="p-values from shuffled data",freq=FALSE,ylim=c(0,4),col=3)
```

## Power
In practice it is quite common to have only three samples. When this is the case. the estimates of the standard deviation is quite variable. Note that we can get very small standard deviation estimates by chance, which turns the t-test large for cases with a very small difference. When performing many tests, as we do in genomcis, these are quite common. So if we focus on the p-value to prioritize genes we may end up cases with very small differences:
```{r}
smallset <- c(1:3,13:15)
smallsett <- rowttests(y[,smallset],group[smallset])
mypar(1,1)
plot(smallsett$dm,-log10(smallsett$p.value),xlab="Effect size",ylab="log -10 (p-value)")
abline(h=-log10(0.01),lty=2)
abline(v=c(-1,1)*log2(1.5),lty=2)
```

In a later module we will leanr statistical techniques from improving the standard deviation estimates.













 
 
 
 















